{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook will download all the data needed for the calculations. The main source is AlphaVantage, which has a curated collection of adjusted  stock prices that are freely downloadable.\n",
    "\n",
    "If you want to download the data, please go through each cell carefully, and mind the comments. This is not meant to be really stable and tested code, and requires your attention to be run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T09:18:48.168314Z",
     "start_time": "2017-09-06T09:18:46.383957Z"
    }
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from modelinter.constants import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll download and save the list of tickers included in the S&P500 as of today\n",
    "# http://data.okfn.org/data/core/s-and-p-500-companies#readme\n",
    "\n",
    "constituents_csv = Paths.datadir.value + 'sp500_constituents.csv'\n",
    "constituents_url = 'https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv'\n",
    "with open(constituents_csv, 'w') as file:\n",
    "    file.write(request.urlopen(constituents_url).read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituents = pd.read_csv(constituents_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the api key from alpha vantage. you need to put your API key here:\n",
    "##      ../data/raw/alphavantage_apikey.txt\n",
    "#or else the code won't work\n",
    "apikey = open(Paths.datadir.value+'alphavantage_apikey.txt', 'r').read().strip()\n",
    "#head and tail of the HTML query we're going to use to download each timeseries\n",
    "head = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol='\n",
    "tail = '&apikey='+apikey+'&datatype=csv&outputsize=full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query the API with the string \"req\"\n",
    "#and return a dataframe with the contents\n",
    "#of the CSV that is sent in response.\n",
    "def query_API(req):\n",
    "    data = request.urlopen(req).read()\n",
    "    df = pd.read_csv(StringIO(data.decode()))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll put temporary results here\n",
    "timeseries = []\n",
    "#and list the tickets that had errors here\n",
    "httperrors = []\n",
    "othererrors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of tickers to download\n",
    "tickers = constituents.Symbol.tolist()[:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#when you're done running the loop below, you can run the line below\n",
    "#which will add the tickers that were not downloaded to the list \"tickers\"\n",
    "#and then re-run the loop to download the missing tickers.\n",
    "tickers = list(set(othererrors)) + list(set(httperrors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is where we'll save the temp results, in case of crashes during execution.\n",
    "temp_timeseries_path = Paths.savedir.value + 'temp_timeseries.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this loop downloads the tickers contained in the list \"tickers\"\n",
    "#and puts the data in the list \"timeseries\".\n",
    "#it's a bit rudimentary, but if you don't need fancy stuff,\n",
    "#it's better not to use it.\n",
    "i,tot = 0, len(tickers)\n",
    "while tickers: #this is so if it breaks you can resume\n",
    "    #select a ticker to download\n",
    "    ticker = tickers[-1]\n",
    "    print(i/tot, end='   ')\n",
    "    i+=1\n",
    "    #build API call\n",
    "    req = head+ticker+tail\n",
    "    try:\n",
    "        #download\n",
    "        df = query_API(req)\n",
    "        #throw away other columns of the dataframe\n",
    "        #we only need adjusted close.\n",
    "        df = df[['timestamp','adjusted_close']]\n",
    "    except KeyError as error:\n",
    "        #if ticker missing from API\n",
    "        #tell me there's an error\n",
    "        tqdm.write('ERROR ON:' + ticker + str(error))\n",
    "        #save in list \"othererrors\"\n",
    "        othererrors.append(ticker)\n",
    "        #pop from list\n",
    "        tickers.pop()\n",
    "        continue\n",
    "    except AttributeError as error:\n",
    "        tqdm.write('ERROR ON:' + ticker + str(error))\n",
    "        othererrors.append(ticker)\n",
    "        tickers.pop()\n",
    "        continue\n",
    "    except HTTPError as error:\n",
    "        #HTTP error usually happens because\n",
    "        #the connection has trouble, so retry\n",
    "        #instead of skipping\n",
    "        tqdm.write('ERROR ON:' + ticker + str(error))\n",
    "        sleep(5)\n",
    "        httperrors.append(ticker)\n",
    "        #don't pop tickers, retry in 5 seconds\n",
    "        continue\n",
    "    #now we downloaded the ticker.\n",
    "    #convert timestamp to pandas format\n",
    "    df.timestamp = pd.to_datetime(df.timestamp, format='%Y-%m-%d')\n",
    "    df = df.set_index('timestamp')\n",
    "    #rename (only) column\n",
    "    df.columns = [ticker]\n",
    "    timeseries.append(df)\n",
    "    #finally pop the ticker from the list\n",
    "    tickers.pop()\n",
    "    #save every 5 tickers in case it crashes\n",
    "    if i%5==0:\n",
    "        pickle.dump(timeseries, open(temp_timeseries_path,'wb'))\n",
    "pickle.dump(timeseries, open(temp_timeseries_path,'wb'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see which tickers caused errors.\n",
    "#you can restart the loop with the tickers that had problems,\n",
    "#(see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(httperrors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(othererrors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pickled timeseries\n",
    "with open(temp_timeseries_path,'rb') as file:\n",
    "    timeseries = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure there are no duplicates\n",
    "timeseries_unique = []\n",
    "timeseries_tickers = []\n",
    "for ts in timeseries:\n",
    "    if ts.columns[0] not in timeseries_tickers:\n",
    "        timeseries_tickers.append(ts.columns[0])\n",
    "        timeseries_unique.append(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download SP500\n",
    "req = head+'^GSPC'+tail\n",
    "data = request.urlopen(req).read()\n",
    "df = pd.read_csv(StringIO(data.decode()))\n",
    "df = df[['timestamp','adjusted_close']]\n",
    "df.timestamp = pd.to_datetime(df.timestamp, format='%Y-%m-%d')\n",
    "df = df.set_index('timestamp')\n",
    "df.columns = ['SP500']\n",
    "\n",
    "timeseries_unique.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download VIX\n",
    "req = head+'^VIX'+tail\n",
    "data = request.urlopen(req).read()\n",
    "df = pd.read_csv(StringIO(data.decode()))\n",
    "df = df[['timestamp','adjusted_close']]\n",
    "df.timestamp = pd.to_datetime(df.timestamp, format='%Y-%m-%d')\n",
    "df = df.set_index('timestamp')\n",
    "df.columns = ['VIX']\n",
    "\n",
    "timeseries_unique.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's join all the timeseries two at a time.\n",
    "#we'll get one big dataframe with all timeseries.\n",
    "jointwo = lambda a,b: a.join(b, how='outer')\n",
    "from functools import reduce\n",
    "\n",
    "timeseries_df = reduce(jointwo, timeseries_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "timeseries_names = [_.columns[0] for _ in timeseries_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apparently, there is some series that has\n",
    "#minute-to-minute data for the day when the\n",
    "#dataset is downloaded, this results in NaN's\n",
    "#for all other daily timeseries.\n",
    "timeseries_df['MMM'][-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's kill all the minute-by-minute datapoints.\n",
    "#limit is the first timestamp of minute-by-minute.\n",
    "#You will have to adjust it manually, because\n",
    "#it's not worth the time to write code for doing it automatically.\n",
    "limit = pd.to_datetime('2017-08-01-00:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll keep all timestamps before it.\n",
    "keep = timeseries_df.index <= limit\n",
    "timeseries_df = timeseries_df.loc[keep,:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#if you want to check that it worked\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(timeseries_df.iloc[:,:].notnull().values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally, save the timeseries to a csv.\n",
    "timeseries_df.to_csv(Paths.datadir.value + 'dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing\n",
    "We're not allowed by the copyright terms to re-publish the data on the repository right away, but we can do so with a processed subset. We'll extract returns and reformat a bit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T09:30:34.352522Z",
     "start_time": "2017-09-06T09:30:33.602872Z"
    }
   },
   "outputs": [],
   "source": [
    "timeseries = pd.read_csv(Paths.datadir.value + 'dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T09:30:34.377527Z",
     "start_time": "2017-09-06T09:30:34.355023Z"
    }
   },
   "outputs": [],
   "source": [
    "#let's keep the last nyears in the dataset\n",
    "tradingyear = 252\n",
    "annualize = np.sqrt(tradingyear)\n",
    "nyears_keep = 5.5\n",
    "timeseries = timeseries.iloc[-int(tradingyear*nyears_keep):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T09:30:34.841620Z",
     "start_time": "2017-09-06T09:30:34.541560Z"
    }
   },
   "outputs": [],
   "source": [
    "#convert dates and set dataframe indices\n",
    "timeseries.timestamp = pd.to_datetime(timeseries.timestamp, format = '%Y-%m-%d')\n",
    "timeseries = timeseries.set_index('timestamp')\n",
    "# move sp500 and vix to first columns, then all stocks sorted alphabetically\n",
    "timeseries = timeseries[\n",
    "        ['SP500', 'VIX']\n",
    "        + list(sorted(\n",
    "            [_ for _ in timeseries.columns.tolist() if (_!='SP500' and _!='VIX')]))\n",
    "        ]\n",
    "# let's kill the ~20% of stocks that didn't exist prior to\n",
    "#earliest date picked in the dataset\n",
    "#and the stocks with >1% missing data\n",
    "#nevermind the biases introduced, they are besides the point\n",
    "#of this analysis.\n",
    "keep = (((timeseries.isnull().sum()/timeseries.shape[0])<.01)\n",
    "       & timeseries.iloc[0,:].notnull().values)\n",
    "timeseries = timeseries.loc[:,keep]\n",
    "#there is a row of almost all missing points in this dataset.\n",
    "#let's interpolate through it.\n",
    "timeseries = timeseries.interpolate(method='linear')\n",
    "#calculate linear returns, remove first row of prices to get same n rows\n",
    "timeseries_returns = ((timeseries.shift(1) - timeseries)/timeseries).iloc[1:,:]\n",
    "timeseries = timeseries.iloc[1:,:]\n",
    "#slices for the dataframe:\n",
    "indices_subset = slice(0,2)\n",
    "stocks_subset = slice(2,None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T09:30:35.115175Z",
     "start_time": "2017-09-06T09:30:35.108674Z"
    }
   },
   "outputs": [],
   "source": [
    "#we will now remove some of the prices and make a csv\n",
    "#that is mostly nan. we'll keep the 100 days interval\n",
    "#centered around the date when the CCAR scenario starts,\n",
    "#because we need those prices for the calculation.\n",
    "#this removal is done to comply with the license on the data\n",
    "#that doesn't allow complete redistribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T09:30:35.703292Z",
     "start_time": "2017-09-06T09:30:35.690790Z"
    }
   },
   "outputs": [],
   "source": [
    "#load CCAR scenario\n",
    "fed_sev_adverse = pd.read_csv(Paths.datadir.value + 'FedSeverelyAdverse.csv')\n",
    "#convert dates and set dataframe indices\n",
    "fed_sev_adverse.columns = [_.lower().replace(' ','_') for _ in fed_sev_adverse.columns]\n",
    "fed_sev_adverse.date = pd.to_datetime(fed_sev_adverse.date, format = '%d-%m-%Y')\n",
    "fed_sev_adverse = fed_sev_adverse.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T09:30:36.210894Z",
     "start_time": "2017-09-06T09:30:36.201892Z"
    }
   },
   "outputs": [],
   "source": [
    "#the interval of days to kill\n",
    "kill1 = all_time = slice(None,\n",
    "                 fed_sev_adverse.index[0]\n",
    "                 - pd.Timedelta(50,'d'))\n",
    "kill2 = all_time = slice(fed_sev_adverse.index[0]\n",
    "                 + pd.Timedelta(50,'d'),\n",
    "                 None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T09:30:52.400131Z",
     "start_time": "2017-09-06T09:30:52.366624Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove most of the prices from previous years,\n",
    "#we only need the a few days\n",
    "timeseries.loc[kill1,stocks_subset] = np.nan\n",
    "timeseries.loc[kill2,stocks_subset] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-06T09:30:56.700991Z",
     "start_time": "2017-09-06T09:30:54.254002Z"
    }
   },
   "outputs": [],
   "source": [
    "timeseries.to_csv(Paths.datadir_free.value + 'timeseries.csv')\n",
    "timeseries_returns.to_csv(Paths.datadir_free.value + 'timeseries_returns.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
